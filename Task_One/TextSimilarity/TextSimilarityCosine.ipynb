{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600519452753",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Cosine Text Similarity\n",
    "\n",
    "This notebook is my attempt to solving Task 1.(ii) where we will compare two given texts and provide a similarity score using cosine similarity.\n",
    "\n",
    "Modules used for task are:\n",
    "+ NLTK (Natural Language Toolkit)\n",
    "+ Numpy (For efficient processing)\n",
    "+ Sklearn (For feature extraction)\n",
    "+ Gensim (For word2vec model)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Text Similarity using Bag of Words/Count Vector\n",
    "Bag of words is one of the more popular forms of vectorization. \n",
    "\n",
    "In this method, we create a `count vector` for each corpus.\n",
    "\n",
    "The `count vector` may either be\n",
    "* Frequency based: Here the count is the frequency of the word in corpus \n",
    "* Presence based : Here the count is binary, present in corpus or not present in corpus \n",
    "\n",
    "Generally the former is preferred over the latter (and is hence implemented below), even though the latter is a bit easier to code.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading necessary modules:\n",
    "\n",
    "Let us first import all the necessary modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "source": [
    "### Preprocessing:\n",
    "\n",
    "Whenever we are given a corpus, we first need to process it.\n",
    "\n",
    "Processing for us includes three steps:\n",
    "- Tokenizing the corpus\n",
    "- Removal of stop words and punctuations\n",
    "- Converting the remaining data to lowercase.\n",
    "\n",
    "This is done using the `process` method below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process(sentence, stop_words=set(stopwords.words('english'))):\n",
    "    '''\n",
    "        Takes some text (sentence) as input (optionally also stop_words)\n",
    "        Gives a set of tokenized words that are not in stop_words\n",
    "        stopwords.words('english') is cast to a set cause lookups in set are O(1)\n",
    "    '''\n",
    "    x = word_tokenize(sentence)\n",
    "    words=[word.lower() for word in x if word.isalpha() and not word in stop_words]\n",
    "    return words"
   ]
  },
  {
   "source": [
    "### Generating count vectors\n",
    "Now that we have some processed data, we can work on generating the `count` vectors. This is done in the `get_vectors` function implemented below\n",
    "\n",
    "To generate the count vectors for two lists - l1 and l2, we need to do to the following:\n",
    "* Get set `all` which is a set of all unique tokens in l1 and l2\n",
    "* For each item in `all`:\n",
    "    - Add it's frequency to l1's count vector\n",
    "    - Add it's frequency to l2's count vector\n",
    "\n",
    "Note: This is because we are using the frequency based approach for generating the `count` vector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vectors(l1, l2):\n",
    "    _all = np.union1d(l1, l2)\n",
    "    v1 = np.zeros(_all.shape)\n",
    "    v2 = np.zeros(_all.shape)\n",
    "    for i in range(len(_all)):\n",
    "        v1[i]=l1.count(_all[i])\n",
    "        v2[i]=l2.count(_all[i])\n",
    "    return v1,v2"
   ]
  },
  {
   "source": [
    "### Taking the cosine\n",
    "\n",
    "Now all that's left is to simply take the cosine. This is easily implemented using numpy.\n",
    "\n",
    "We can easily get the cosine between two vectors using the dot product. \n",
    "\n",
    "This is done as: `cos(x) = A.B/|A|.|B|` \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    mod = np.linalg.norm(vec1)*np.linalg.norm(vec2)\n",
    "    if mod == 0:\n",
    "        return 0\n",
    "    return x@y/mod"
   ]
  },
  {
   "source": [
    "### Driver Code:\n",
    "\n",
    "We can now test out the cosine similarity between two texts using the driver code below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The cosine similarity is: 0.7647058823529411\n"
    }
   ],
   "source": [
    "sentence_1 = input(\"Enter some text\")\n",
    "sentence_2 = input(\"Enter more text\")\n",
    "tokens_1 = process(sentence_1)\n",
    "tokens_2 = process(sentence_2)\n",
    "x,y = get_vectors(tokens_1, tokens_2)\n",
    "print(\"The cosine similarity is:\", cosine(x,y))"
   ]
  },
  {
   "source": [
    "## Text Similarity using TF-IDF\n",
    "TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency. This is another very common algorithm for vectorization.\n",
    "\n",
    "It is different from our above implementation because of the \"IDF\" part of it.\n",
    "\n",
    "IDF or Inverse Document Frequency is a scoring of how rare the word is across documents.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading necessary modules:\n",
    "We will be using `sklearn`'s `TfidfVectorizer` to convert a given text in vector form.\n",
    "We will then go on to apply our cosine function to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "source": [
    "### Preprocessing:\n",
    "We will be heavily relying on the `process` function that we defined earlier to process our sentence. But since the `TfidfVectorizer` needs a string to fit, we will also convert the list we get after processing into a string of words.\n",
    "\n",
    "This is done by:\n",
    "* Apply `process` function on string\n",
    "* Join the resulting list with spaces in between words\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(sentence):\n",
    "    x = ' '.join(process(sentence))\n",
    "    return x"
   ]
  },
  {
   "source": [
    "### Generating Vectors\n",
    "We will directly be using the `TfidfVectorizer` to vectorize our sentence.\n",
    "\n",
    "But this has one small problem. The feature vectors that we extract from the `TfidfVectorizer` are in the form of a sparse matrix. We should probably also convert them into numpy arrays in the vector generation step itself.\n",
    "\n",
    "Note: We can also use `sklearn.metrics.pairwise.cosine_similarity` to calculate the cosine similarity on the sparse matrix itself. But since we have already implemented our own cosine function, we should use that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_generate_vectors(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    tfidf_vectorizer.fit(corpus)\n",
    "    feature_vectors = tfidf_vectorizer.transform(corpus)\n",
    "    x,y = feature_vectors[0].toarray(), feature_vectors[1].toarray()\n",
    "    return x.reshape(-1,),y.reshape(-1,)"
   ]
  },
  {
   "source": [
    "### Taking the cosine\n",
    "\n",
    "Now all that's left is to simply take the cosine. We have already implemented a function above to get the cosine between two vectors using the dot product.\n",
    "\n",
    "As a reminder, it is implemented as: `cos(x) = A.B/|A|.|B|` \n",
    "\n",
    "And the function name is `cosine`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Driver Code:\n",
    "\n",
    "We can now test out the cosine similarity between two texts using the driver code below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.40724772010124605"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "sentence_1 = input(\"Enter some text\")\n",
    "sentence_2 = input(\"Enter more text\")\n",
    "corpus = [pre_process(sentence_1), pre_process(sentence_2)]\n",
    "x,y = tfidf_generate_vectors(corpus)\n",
    "cosine(x,y)"
   ]
  },
  {
   "source": [
    "## Text Similarity Using Word2Vec Embeddings (with GloVe Data)\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. It takes as its input a text and produces a vector space of several hundred dimensions. \n",
    "\n",
    "For this case, that is all we need to know about it. We will use Word2Vec to vectorise our words and then get cosine similarity by comparing those vectors.\n",
    "\n",
    "We will be using Stanford's `GloVe (Global Vectors for Word Repesentation)` data for the model (Just because file was comparatively smaller to download)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading necessary modules:\n",
    "Let us start off by loading in the necessary modules along with the model.\n",
    "\n",
    "We will use the `genism` library to load in the required model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = '../../data/taskone/ii/glove.6B.300d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "source": [
    "### Preprocessing\n",
    "\n",
    "Most of our preprocessing is already done in the `process` method that we defined earlier. We will mostly be using the same for this implementation as well.\n",
    "\n",
    "One thing to note is that our model has a vocabulary, so if we try to get the similarity (or vectors) for tokens that are not in the vocabulary we will end up with an error.\n",
    "\n",
    "It will be a good idea to get rid of such words in preprocessing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_process(sentence_1, model=model):\n",
    "    x= process(sentence_1)\n",
    "    x = [i for i in x if i in model.vocab]\n",
    "    return x"
   ]
  },
  {
   "source": [
    "### Generating the vectors:\n",
    "\n",
    "One thing to note is that our model has a method called `similarity` which gives us the similarity between two *words* if they are in it's vocabulary. That's not very helpful when we need to find similarity between two texts.\n",
    "\n",
    "One way to counter this problem would be to run word2vec on the words in both sentences, sum up the vectors in the one sentence, sum up the vectors in the other sentence, and then find the cosin similarity between these vectors. By summing them up instead of doing a word-wise difference, you'll at least not be subject to word order.\n",
    "\n",
    "This is the method that we will implement, and we will do this summing up as a part of our vector genertion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_generate_vector(token):\n",
    "    x = 0\n",
    "    for i in token:\n",
    "        x += model.get_vector(i)\n",
    "    return x"
   ]
  },
  {
   "source": [
    "### Taking the cosine\n",
    "\n",
    "Now all that's left is to simply take the cosine. We have already implemented a function above to get the cosine between two vectors using the dot product.\n",
    "\n",
    "As a reminder, it is implemented as: `cos(x) = A.B/|A|.|B|` \n",
    "\n",
    "And the function name is `cosine`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Driver Code:\n",
    "\n",
    "We can now test out the cosine similarity between two texts using the driver code below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.95129585"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "sentence_1 = input(\"Enter some text\")\n",
    "sentence_2 = input(\"Enter more text\")\n",
    "tokens_1 = w2v_process(sentence_1)\n",
    "tokens_2 = w2v_process(sentence_2)\n",
    "x, y = w2v_generate_vector(tokens_1),  w2v_generate_vector(tokens_2)\n",
    "cosine(x,y)"
   ]
  }
 ]
}