{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600609519992",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Twitter Sentiment Analysis:\n",
    "\n",
    "Sentiment Analysis means analyzing the sentiment of a given text or document and categorizing the text/document into a specific class or category. In our case, we will be classifying tweets as either positive or negative.\n",
    "\n",
    "We will be using the following modules for this project:\n",
    "* NLTK (For implementation based on nltk)\n",
    "* Numpy, re, string (For efficient processing)\n",
    "* tweepy (For fetching tweets from the api)\n",
    "* textblob (For implementation based on textblob)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sentiment Analysis using NLTK\n",
    "\n",
    "Here we will be training a model called `NaiveBayesClassifier` to perform supervised classification. The first thing to do is to import everything that we will be needing for this project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import NaiveBayesClassifier, classify"
   ]
  },
  {
   "source": [
    "Let us first setup a tweepy API object so that we can fetch tweets easily.\n",
    "\n",
    "Let us also function to fetch tweets by a tweet id. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open('../../data/taskone/i/TwitterAPI.txt')\n",
    "ids = fh.read().split('\\n')\n",
    "API_KEY = ids[0]\n",
    "API_KEY_SECRET = ids[1]\n",
    "ACCESS_TOKEN = ids[2]\n",
    "ACCESS_TOKEN_SECRET = ids[3]\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_KEY_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(_id, authenticated_api=api):\n",
    "    tweet = authenticated_api.get_status(_id)\n",
    "    return tweet.text"
   ]
  },
  {
   "source": [
    "The twitter_samples from nltk.corpus has 3 files:\n",
    "* positive_tweets.json: A file containing 5000 positive tweets\n",
    "* negative_tweets.json: A file containing 5000 negative tweets\n",
    "* tweets.20150430-223406.json: A file containing 20000 tweets\n",
    "\n",
    "We will be using the first two files to train our `NaiveBayesClassifier`\n",
    "\n",
    "Let's load in the data into lists using the `twitter_samples.strings` method\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "source": [
    "The first order of business that we have to attend to is cleaning and tokenizing the corpus (which is a tweet in this case)\n",
    "\n",
    "We will do that by writing a function specifically to clean a tweet (and tokenize it eventually)\n",
    "\n",
    "To clean the tweet, we will use the following criteria:\n",
    "- Remove stock market symbols like $\n",
    "- Remove hyperlinks and hashtags\n",
    "- Remove stop words\n",
    "- Remove punctuation\n",
    "- Stem each token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                               strip_handles=True, \n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweet_clean = []\n",
    "    stemmer = PorterStemmer()\n",
    "    sw = stopwords.words('english')\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in sw and\n",
    "            word not in string.punctuation):\n",
    "            tweet_clean.append(stemmer.stem(word))\n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['hello',\n 'hope',\n 'good',\n 'day',\n 'sunni',\n 'outsid',\n 'realli',\n 'sunni',\n ':)',\n 'good',\n 'morn']"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "random_tweet = \"RT @Twitter Hello There! Hope you are having a good day. It's so sunny outside. It really is sunny. I have $5 :) #good #morning http://www.youtube.com/watch?\"\n",
    "clean(random_tweet)"
   ]
  },
  {
   "source": [
    "Now we need to make a feature extractor. We will be using creating a simple bag of words function called `BoW` which will return a dictionary of booleans.\n",
    "\n",
    "Note: We can't use a frequency based vectorization like in the Text Similarity project because we do not have a reference vector in this case. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(tweet_clean):\n",
    "    words_dictionary = dict([word, True] for word in tweet_clean)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "source": [
    "We should now focus on creating the feature sets from our `pos_tweet_set` and `neg_tweet_set`\n",
    "\n",
    "We will be doing this by looping over each tweet in both sets and:\n",
    "- Cleaning the tweet\n",
    "- Extracting the features from the tokens(using BoW function)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features = []\n",
    "neg_features = []\n",
    "for tweet in pos_tweets:\n",
    "    tweet = clean(tweet)\n",
    "    pos_features.append((BoW(tweet), 1))\n",
    "for tweet in neg_tweets:\n",
    "    tweet = clean(tweet)\n",
    "    neg_features.append((BoW(tweet), 0))"
   ]
  },
  {
   "source": [
    "Our `pos_features` is a list of tuples, where the first element in the tuple is the bag of words for a particular tweet and the second element is it's sentiment. The same holds for `neg_features`.\n",
    "\n",
    "We have chosen `1` to denote positive sentiment and `0` to denote negative sentiment in this project. Hence `pos_features` has `1` as the second element in every tuple, and `neg_features` has `0` as the second element in every tuple.\n",
    "\n",
    "This basically implies that we now have labelled data that we can split into training and testing data. `pos_features` has all labelled data for positive tweets, and `neg_features` has all labelled data for negative tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pos_features[:100] + neg_features[:100]\n",
    "train_set = pos_features[100:] + neg_features[100:]"
   ]
  },
  {
   "source": [
    "We are now ready to train our model.\n",
    "\n",
    "We define a `NaiveBayesClassifier` below and train it on the `train_set`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "source": [
    "We can now test out `NaiveBayesClassifier` on some dummy data. Let's say someone tweets that they had a bad day today.\n",
    "\n",
    "NOTE: 0 Implies a negative sentiment (Expected output)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dummy_tweet = \"RT @Twitter I had such a bad day today :( #bad #morning\"\n",
    "dummy_bag = BoW(clean(dummy_tweet))\n",
    "classifier.classify(dummy_bag)"
   ]
  },
  {
   "source": [
    "Recall, `0` denotes a negative sentiment. Our model has caught the negative sentiment in the dummy tweet correctly and given us an output of `0`\n",
    "\n",
    "We should at this point try to check the probablity of our prediction (or classification).\n",
    "\n",
    "We can do this easily using `NaiveBayesClassifier.prob_classify` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.9995324354143069, 0.00046756458569199155)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "prob = classifier.prob_classify(dummy_bag)\n",
    "prob.prob(0), prob.prob(1) #Probability of 0 (negative), Probability of 1"
   ]
  },
  {
   "source": [
    "As we can see, the probability of our tweet having a negative sentiment according to the `NaiveBayesClassifier` is 0.9995 (99.95%)\n",
    "\n",
    "We can also test the accuracy of this tweet on our `test_set` and see how well it performs.\n",
    "\n",
    "This can be done using `classify.accuracy` on our trained `NaiveBayesClassifier` and `test_set`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.97"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "source": [
    "This shows us a very good accuracy score of 97%!\n",
    "\n",
    "That may just be the courtesy of us having a small `test_set`, but we now know that it does correctly classify tweets to a decent extent.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can now move on to defining a `get_sentiment` function that sortof acts as a wrapper around our previously defined functions.\n",
    "\n",
    "It will take a raw tweet as its input and give us it's sentiment and the probability which it used to decide the sentiment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweet):\n",
    "    clean_data = clean(tweet)\n",
    "    bag = BoW(clean_data)\n",
    "    ans = classifier.classify(bag)\n",
    "    prob =  classifier.prob_classify(bag)\n",
    "    if ans==0:\n",
    "        return \"Negative Sentiment\", prob.prob(0)\n",
    "    return \"Positive Sentiment\", prob.prob(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('Negative Sentiment', 0.9995324354143069)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "get_sentiment(dummy_tweet)"
   ]
  },
  {
   "source": [
    "As we can see, this gives us the expected results. So we can now write a driver code for this.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The tweet has Positive Sentiment \nThis was predicted with 0.6957860762007811 probability\n"
    }
   ],
   "source": [
    "_id = input(\"Enter a tweet ID\")\n",
    "tweet = get_tweet(_id)\n",
    "s,p = get_sentiment(tweet)\n",
    "print(\"The tweet has\", s, \"\\nThis was predicted with\", p, \"probability\")"
   ]
  },
  {
   "source": [
    "This was sentiment analysis using the `nltk` module.\n",
    "We also happen to have a module called `textblob` which is built on top of `nltk`.\n",
    "We will also explore an implementation based on that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sentiment Analysis using textblob\n",
    "\n",
    "Here we will be using the textblob library to extract the sentiment in a given text. The first thing to do is to import the required libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "source": [
    "A `TextBlob` is an object that is initialised with a string. It has a very convenient property called `sentiment` which we can access to easily figure out the sentiment in the string used to initialise it.\n",
    "\n",
    "In our case, this string will be the tweet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before we create the TextBlob object, we will need a cleaned string.\n",
    "\n",
    "This will mainly be done using the `clean` function we created above for the `nltk` implementation, but we will need to convert the cleaned data back to a string.\n",
    "\n",
    "This is because the TextBlob object needs to be initialised with a string, and not a list of words.\n",
    "\n",
    "We will test this out on `dummy_tweet` that we defined above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'bad day today :( bad morn'"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "def tb_clean(tweet):\n",
    "    return ' '.join(clean(tweet))\n",
    "tb_clean(dummy_tweet)"
   ]
  },
  {
   "source": [
    "As we can see, it returns a string of cleaned words (As expected.)\n",
    "\n",
    "Now all we need to do is initialise a `TextBlob` object with this clean string and take a look at it's `sentiment` property.\n",
    "\n",
    "The `sentiment` property of a `TextBlob` is an object itself and has two properties of its own.\n",
    "* sentiment.polarity\n",
    "* sentiment.subjectivity\n",
    "\n",
    "The polarity indicates the sentiment in the string. So we will use that.\n",
    "\n",
    "At this point, since we don't need to generate any vectors for our string or train any models, we can directly start writing our final wrapper function (as we did at the end of our `nltk` implementation)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Negative Sentiment'"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "def tb_sentiment(tweet):\n",
    "    tweet = tb_clean(tweet)\n",
    "    tb = TextBlob(tweet)\n",
    "    if tb.sentiment.polarity > 0:\n",
    "        return 'Positive Sentiment'\n",
    "    else:\n",
    "        return 'Negative Sentiment'\n",
    "tb_sentiment(dummy_tweet)"
   ]
  },
  {
   "source": [
    "We can now simply write a driver code and conclude our sentiment analysis project."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The tweet has Positive Sentiment\n"
    }
   ],
   "source": [
    "_id = input(\"Enter a tweet ID\")\n",
    "tweet = get_tweet(_id)\n",
    "s = tb_sentiment(tweet)\n",
    "print(\"The tweet has\", s)"
   ]
  },
  {
   "source": [
    "# Conclusion:\n",
    "We have thus, explored two different ways of sentiment analysis using two different libraries in python.\n",
    "* `nltk`\n",
    "* `textblob`\n",
    "\n",
    "## NLTK \n",
    "We first started off with cleaning our tweets\n",
    "\n",
    "We then went on to implement a feature extraction function to generate a bag of words\n",
    "\n",
    "We then trained a classifier called `NaiveBayesClassifier` to perform supervised classification.\n",
    "\n",
    "## Textblob \n",
    "We first started off with cleaning our tweets\n",
    "\n",
    "We did not need to implement our own feature extraction function or train our own classifier\n",
    "\n",
    "We initialised a TextBlob object and accessed it's `sentiment` property"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}